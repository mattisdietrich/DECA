{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for training with decoupled shape parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decalib.deca import DECA\n",
    "from decalib.datasets import datasets \n",
    "from decalib.utils import util\n",
    "from decalib.utils.config import cfg as deca_cfg\n",
    "from skimage.io import imread\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import cv2\n",
    "import trimesh\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get the Shape parameters for an image -> we need this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_params(image_path: str, device='cuda'):\n",
    "    \"\"\"Function for getting the shape paramters of an image\n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    deca = DECA()\n",
    "\n",
    "    #Get the data in the right format\n",
    "    test_img = datasets.TestData(image_path)\n",
    "    deca_cfg.model.use_tex = False\n",
    "    deca_cfg.model.extract_tex = False\n",
    "    #Get the codedictionarys\n",
    "    for i in tqdm(range(len(test_img))):\n",
    "        name = test_img[i]['imagename']\n",
    "        images_l = test_img[i]['image'].to(device)[None,...]\n",
    "        with torch.no_grad(): \n",
    "            codedict = deca.encode(images_l)\n",
    "            \n",
    "    return codedict['shape']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every image, we need to get the shape parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_shape_params('/home/dietrich/Testing/DECA/DECA/TestSamples/Brenken_Rudolf/2017-10-24_00-00_Brenken-Rudolf_06/2017-10-24_001.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = '/home/dietrich/Testing/DECA/Dataset/VGGFACE/Images/VGGface2_None_norm_512_true_bygfpgan'\n",
    "lmks_folder = '/home/dietrich/Testing/DECA/Dataset/VGGFACE/lmks_train'\n",
    "shape_folder = '/home/dietrich/Testing/DECA/Dataset/VGGFACE/shape_train'\n",
    "img_name_list = []\n",
    "\n",
    "total_steps = 9279\n",
    "counter = 0\n",
    "\n",
    "progress_slider = widgets.IntSlider(min=0, max=total_steps, value=counter, description='Progress')\n",
    "logging.basicConfig(filename='/home/dietrich/Testing/DECA/Dataset/VGGFACE/lmks_and_shape_processing_6_imgs.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def update_slider(value):\n",
    "    progress_slider.value = value\n",
    "display(progress_slider)\n",
    "\n",
    "lms_478_to_68 = [\n",
    "                162,234,93,58,172,136,149,148,152,377,378,365,397,288,323,454,389,71,63,105,66,107,336,\n",
    "                296,334,293,301,168,197,5,4,75,97,2,326,305,33,160,158,133,153,144,362,385,387,263,373,\n",
    "                380,61,39,37,0,267,269,291,405,314,17,84,181,78,82,13,312,308,317,14,87\n",
    "            ]\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "                static_image_mode = True,\n",
    "                max_num_faces = 1,\n",
    "                refine_landmarks=False,\n",
    "                )\n",
    "\n",
    "expected_landmarks_count = 34 # Half should be detected\n",
    "# Through all folders in the Dataset Path\n",
    "for individ__name in os.listdir(images_folder):\n",
    "    individ__path = os.path.join(images_folder, individ__name)\n",
    "\n",
    "    if os.path.isdir(individ__path):\n",
    "        lmks_individ_path = os.path.join(lmks_folder, individ__name)\n",
    "        if not os.path.exists(lmks_individ_path):\n",
    "            os.makedirs(lmks_individ_path)\n",
    "        # e.g. K - number of images per person\n",
    "        img_num = 6\n",
    "        # counting valid images and break, if enough images are taken\n",
    "        img_list_individual = []\n",
    "        # For one individual (one folder) going through the images, until 6 are detected\n",
    "        for image_name in os.listdir(individ__path):\n",
    "            image_path = os.path.join(individ__path, image_name)\n",
    "\n",
    "            if image_name.endswith('.jpg'):\n",
    "                fn, ext = os.path.splitext(os.path.basename(image_name))\n",
    "\n",
    "                #Process Mediapipe\n",
    "                image = cv2.imread(image_path)\n",
    "                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('uint8')\n",
    "\n",
    "                results = face_mesh.process(image_rgb)\n",
    "                \n",
    "                # Excactly one person should be detected, else the image is not used\n",
    "                if results.multi_face_landmarks and len(results.multi_face_landmarks) == 1:\n",
    "                    landmarks = results.multi_face_landmarks[0].landmark\n",
    "                    selected_lm = [landmarks[i] for i in lms_478_to_68] \n",
    "                    selected_lm = [[lm.x * image.shape[1], lm.y * image.shape[0]] for lm in selected_lm]\n",
    "\n",
    "                    # Half of the face need to be detected, else another image is used\n",
    "                    if len(selected_lm) >= expected_landmarks_count:\n",
    "                        np.save(os.path.join(lmks_individ_path, fn + \"_68kpts.npy\") ,selected_lm)\n",
    "                        img_list_individual.append(os.path.join(individ__name, fn))\n",
    "                    else:\n",
    "                        logging.info(f'Not used: Detected less than 35 Landmarks: {individ__path}/{image_name}')\n",
    "                else:\n",
    "                    logging.info(f'Not used: Detected none or more than one face: {individ__path}/{image_name}')\n",
    "\n",
    "            #If we got 6 Images, the next individual will be landmarked\n",
    "            if len(img_list_individual) == img_num:\n",
    "                img_name_list.append(img_list_individual)\n",
    "                np.save(\"/home/dietrich/Testing/DECA/Dataset/VGGFACE/\"+\"data_names_6_per_individual\", img_name_list)\n",
    "                break\n",
    "    counter +=1\n",
    "    update_slider(counter)\n",
    "    \n",
    "print(\"Landmark Detection and Image Selection Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, if getting shape parameters is working the right way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = '/home/dietrich/Testing/DECA/Dataset/VGGFACE/data_names_6_per_individual.npy'\n",
    "data_lines = np.load(datafile).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_list = []\n",
    "base_path = '/home/dietrich/Testing/DECA/Dataset/VGGFACE/Images/VGGface2_None_norm_512_true_bygfpgan'\n",
    "\n",
    "idx = 0\n",
    "for i in range(6):\n",
    "    name = data_lines[idx, i]\n",
    "    image_path = os.path.join(base_path, name + '.jpg') \n",
    "    shape_params = get_shape_params(image_path)\n",
    "    shape_list.append(shape_params)\n",
    "\n",
    "shape_2d = torch.stack(shape_list)\n",
    "mean_shape = torch.mean(shape_2d, dim=0).cpu().detach().numpy()\n",
    "mean_shape_list = [mean_shape for _ in shape_list]\n",
    "\n",
    "\n",
    "shapearray = torch.from_numpy(np.array(mean_shape_list)).type(dtype = torch.float32) #K,100\n",
    "\n",
    "print(shapearray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decalib.deca import DECA\n",
    "from decalib.trainer import Trainer\n",
    "from decalib.utils.config_train import config_train as cfg\n",
    "import yaml\n",
    "import shutil\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(cfg.output_dir, cfg.train.log_dir), exist_ok=True)\n",
    "os.makedirs(os.path.join(cfg.output_dir, cfg.train.vis_dir), exist_ok=True)\n",
    "os.makedirs(os.path.join(cfg.output_dir, cfg.train.val_vis_dir), exist_ok=True)\n",
    "with open(os.path.join(cfg.output_dir, cfg.train.log_dir, 'full_config.yaml'), 'w') as f:\n",
    "    yaml.dump(cfg, f, default_flow_style=False)\n",
    "shutil.copy(cfg.cfg_file, os.path.join(cfg.output_dir, 'config.yaml'))\n",
    "    \n",
    "# cudnn related setting\n",
    "cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# start training\n",
    "# deca model\n",
    "cfg.rasterizer_type = 'pytorch3d'\n",
    "deca = DECA(cfg)\n",
    "trainer = Trainer(model=deca, config=cfg)\n",
    "\n",
    "## start train\n",
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
