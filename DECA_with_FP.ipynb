{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECA Model with Facialis Parese\n",
    "\n",
    "1. Follow README_Install_Mattis.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_HOME=$CONDA_PREFIX python demos/demo_reconstruct.py -i TestSamples/examples --saveDepth True --saveObj True --rasterizer_type pytorch3d\n",
    "\n",
    "image = mpimg.imread(\"TestSamples/examples/results/alfw1_vis.jpg\")\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw Testing - facialis parese patient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facialis Parese FLAME model with disassembled sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Loading Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "input_path = \"TestSamples/Brenken_Rudolf/\"\n",
    "\n",
    "input_image = \"2017-04-12_007.png\"\n",
    "\n",
    "# Extrahiere den Dateinamen mit der Erweiterung\n",
    "filename_with_extension = os.path.basename(input_image)\n",
    "\n",
    "# Entferne die Dateierweiterung\n",
    "filename_without_extension, extension = os.path.splitext(filename_with_extension)\n",
    "\n",
    "# Teile die Zeichenkette am Unterstrich, um das gewÃ¼nschte Datum und die Nummer zu erhalten\n",
    "date_part, number_part = filename_without_extension.split(\"_\")\n",
    "\n",
    "print(\"Datum:\", date_part)\n",
    "print(\"Nummer:\", number_part)\n",
    "\n",
    "image = cv2.imread(input_path+input_image)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#==========Plot==========\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Decoupling images to left and right side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "#Using Mediapipe to decouple the image to left and right side via the midpoints of the eyes to make it more \"middle\" than just using the image shape\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh()\n",
    "\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('uint8')\n",
    "\n",
    "results = face_mesh.process(image_rgb)\n",
    "landmarks = results.multi_face_landmarks[0].landmark\n",
    "\n",
    "left_eye_inner = np.array([landmarks[159].x, landmarks[159].y, landmarks[159].z])\n",
    "right_eye_inner = np.array([landmarks[386].x, landmarks[386].y, landmarks[386].z])\n",
    "\n",
    "eye_midpoint = (left_eye_inner + right_eye_inner) / 2\n",
    "\n",
    "eye_midpoint_pixel = (int(eye_midpoint[0] * image.shape[1]), int(eye_midpoint[1] * image.shape[0]))\n",
    "\n",
    "print(eye_midpoint_pixel)\n",
    "\n",
    "right_half = image[:, :eye_midpoint_pixel[0], :]\n",
    "left_half = image[:, eye_midpoint_pixel[0]:, :]\n",
    "\n",
    "#==========Plot==========\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(left_half)\n",
    "plt.title('Left Half')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(right_half)\n",
    "plt.title('Right half')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Flipping and adjusting the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_half_flipped = np.flip(left_half, axis=1)\n",
    "right_half_flipped = np.flip(right_half, axis=1)\n",
    "\n",
    "left_half_full_image = np.concatenate([left_half_flipped, left_half], axis=1)\n",
    "right_half_full_image = np.concatenate([right_half, right_half_flipped], axis=1)\n",
    "\n",
    "left_image_path = input_path + \"left_half_full_image.png\"\n",
    "right_image_path = input_path + \"right_half_full_image.png\"\n",
    "\n",
    "plt.imsave(left_image_path, left_half_full_image)\n",
    "plt.imsave(right_image_path, right_half_full_image)\n",
    "\n",
    "#==========Plot==========\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(left_half_full_image)\n",
    "plt.title('Left Half mirrored')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(right_half_full_image)\n",
    "plt.title('Right half mirrored')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Adjusting FLAME to both sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"TestSamples/Brenken_Rudolf/results/leftright\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_HOME=$CONDA_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--iscrop False , because else the image would be cropped and the face would not be full\n",
    "%run demos/demo_reconstruct.py -i $left_image_path --saveDepth True --savefolder $output_path --saveObj True --rasterizer_type pytorch3d --render_orig True --iscrop False\n",
    "\n",
    "image = mpimg.imread(output_path + \"/left_half_full_image_vis_original_size.jpg\")\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run demos/demo_reconstruct.py -i $right_image_path --saveDepth True --savefolder $output_path --saveObj True --rasterizer_type pytorch3d --render_orig True\n",
    "\n",
    "image = mpimg.imread(output_path + \"/right_half_full_image_vis_original_size.jpg\")\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Bringing both sides together and building a coherent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blending_sides(vertices_left, vertices_right, link:str):\n",
    "\n",
    "    vertice_exp_lr = np.zeros((5023, 3))\n",
    "    template_vertices = vertices_right\n",
    "    vertice_max = template_vertices[:,0].max()\n",
    "    vertice_min = template_vertices[:,0].min()\n",
    "    vertice_diff = vertice_max - vertice_min\n",
    "    print(vertice_diff)\n",
    "\n",
    "    # Iterate through each vertex and apply the specified blending method\n",
    "    for i in range(0, 5023):\n",
    "        rel_dist = (template_vertices[i, 0] - vertice_min) / vertice_diff\n",
    "        if link == \"linear\":\n",
    "            vertice_exp_lr[i] = rel_dist * vertices_right[i] + (1 - rel_dist) * vertices_left[i]\n",
    "        elif link == \"exponential\":\n",
    "            rel_dist_scaled = rel_dist * 2 - 1  # Skaliere von 0-1 zu -1-1, um symmetrische Exponentialfunktion zu erhalten\n",
    "            rel_dist_transformed = (np.exp(rel_dist_scaled) - 1) / (np.exp(1) - 1)  # Hier exponentielle Transformation von -1-1 auf 0-1\n",
    "            vertice_exp_lr[i] = rel_dist_transformed * vertices_right[i] + (1 - rel_dist_transformed) * vertices_left[i]\n",
    "        elif link == \"binary\":\n",
    "            vertice_exp_lr[i] = vertices_right[i]+0.1*vertices_left[i] if rel_dist > 0.5 else vertices_left[i]\n",
    "            \n",
    "    return vertice_exp_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrender\n",
    "import trimesh\n",
    "def viewer(vertices, faces):\n",
    "    \"\"\"Interactive visualization via Pyrender Viewer of a (face) mesh\n",
    "\n",
    "    Args:\n",
    "        vertices (np.ndarray): 3D vertices of the mesh.\n",
    "        faces (np.ndarray): Faces defining the mesh.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    vertex_colors = np.ones([vertices.shape[0], 4]) * [1.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "    tri_mesh = trimesh.Trimesh(vertices, faces, vertex_colors=vertex_colors)\n",
    "    tri_mesh.export('output_mesh.obj')\n",
    "    mesh = pyrender.Mesh.from_trimesh(tri_mesh)\n",
    "    scene = pyrender.Scene()\n",
    "    scene.add(mesh)\n",
    "    pyrender.Viewer(scene, use_raymond_lighting=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_left = np.loadtxt(output_path+\"/left_half_full_image/left_half_full_image_vertice.txt\")\n",
    "vertices_right = np.loadtxt(output_path+\"/right_half_full_image/right_half_full_image_vertice.txt\")\n",
    "faces = np.loadtxt(output_path+\"/left_half_full_image/left_half_full_image_faces.txt\")\n",
    "\n",
    "#Problem exponential und linear: Ãnderung kaum zu erkennen\n",
    "vertice_lr = blending_sides(vertices_left, vertices_right, link=\"binary\")\n",
    "\n",
    "vertex_colors = np.ones([vertice_lr.shape[0], 4]) * [1.0, 1.0, 1.0, 1.0]\n",
    "tri_mesh = trimesh.Trimesh(vertice_lr, faces, vertex_colors=vertex_colors)\n",
    "viewer(vertice_lr, faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a shape and pose model and only using the expression parameters for the sides "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Getting a template Model without specified parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Realized: Wen need to get the expression parameters and take them to FLAME_PyTorch, so the other way around; it's easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Getting the expression parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decalib.deca import DECA\n",
    "from decalib.datasets import datasets \n",
    "from decalib.utils import util\n",
    "from decalib.utils.config import cfg as deca_cfg\n",
    "from skimage.io import imread\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Beispielaufruf\n",
    "image_path = left_image_path\n",
    "device = 'cuda'\n",
    "deca = DECA(config = deca_cfg, device='cuda')\n",
    "\n",
    "testdata = datasets.TestData(image_path)\n",
    "deca = DECA(config = deca_cfg, device=device)\n",
    "# for i in range(len(testdata)):\n",
    "for i in tqdm(range(len(testdata))):\n",
    "    name = testdata[i]['imagename']\n",
    "    images = testdata[i]['image'].to(device)[None,...]\n",
    "    with torch.no_grad():\n",
    "        codedict = deca.encode(images)\n",
    "\n",
    "# FÃ¼hren Sie das DECA-Modell aus, um die Expressionsparameter zu erhalten\n",
    "expressions_params = codedict['exp']\n",
    "\n",
    "# Drucken Sie die Expressionsparameter\n",
    "print(\"Expressionsparameter:\")\n",
    "print(expressions_params)\n",
    "\n",
    "torch.save(expressions_params, 'expressions_params_left.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decalib.deca import DECA\n",
    "from decalib.datasets import datasets \n",
    "from decalib.utils import util\n",
    "from decalib.utils.config import cfg as deca_cfg\n",
    "from skimage.io import imread\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Beispielaufruf\n",
    "image_path = right_image_path\n",
    "device = 'cuda'\n",
    "deca = DECA(config = deca_cfg, device='cuda')\n",
    "\n",
    "testdata = datasets.TestData(image_path)\n",
    "deca = DECA(config = deca_cfg, device=device)\n",
    "# for i in range(len(testdata)):\n",
    "for i in tqdm(range(len(testdata))):\n",
    "    name = testdata[i]['imagename']\n",
    "    images = testdata[i]['image'].to(device)[None,...]\n",
    "    with torch.no_grad():\n",
    "        codedict = deca.encode(images)\n",
    "\n",
    "# FÃ¼hren Sie das DECA-Modell aus, um die Expressionsparameter zu erhalten\n",
    "expressions_params = codedict['exp']\n",
    "\n",
    "# Drucken Sie die Expressionsparameter\n",
    "print(\"Expressionsparameter:\")\n",
    "print(expressions_params)\n",
    "\n",
    "torch.save(expressions_params, 'expressions_params_right.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DePa\n",
    "\n",
    "!CUDA_HOME=$CONDA_PREFIX\n",
    "\n",
    "image_path = \"TestSamples/Brenken_Rudolf/2017-10-24_00-00_Brenken-Rudolf_06/2017-10-24_001.jpg\"\n",
    "results_path = \"TestResults/testing/\"\n",
    "\n",
    "DePa.decouple(image_path, results_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decalib.models.encoders import ResnetEncoder\n",
    "import numpy as np\n",
    "from decalib.datasets import datasets \n",
    "import torch\n",
    "import os\n",
    "from decalib.utils import util\n",
    "\n",
    "def get_shape_params(image_path: str, device='cuda'):\n",
    "    \"\"\"Function for getting the shape paramters of an image\n",
    "    \"\"\"\n",
    "    \n",
    "    testdata__ = datasets.TestData(image_path)\n",
    "    for i in range(len(image_path)):\n",
    "        imgs_batch = testdata__[1]['image'].to('cuda')[None,...]\n",
    "        codedict = encode(imgs_batch)\n",
    "            \n",
    "    return codedict['shape']\n",
    "\n",
    "n_shape = 100\n",
    "n_tex = 50\n",
    "n_exp = 50\n",
    "n_pose = 6\n",
    "n_cam = 3\n",
    "n_light = 27\n",
    "\n",
    "model_path = \"/home/dietrich/Testing/DECA/DECA/data/deca_model.tar\"\n",
    "num_params = n_shape+n_tex+n_exp+n_pose+n_cam+n_light\n",
    "E_flame  = ResnetEncoder(outsize=num_params).to('cuda')\n",
    "param_dict = {'shape': n_shape, 'tex': n_tex, 'exp': n_exp, 'pose': n_pose, 'cam': n_cam, 'light': n_light}\n",
    "checkpoint = torch.load(model_path)\n",
    "util.copy_state_dict(E_flame.state_dict(), checkpoint['E_flame'])\n",
    "E_flame.eval()\n",
    "\n",
    "def encode(imgs_batch):\n",
    "        with torch.no_grad():\n",
    "                parameters = E_flame(imgs_batch)\n",
    "        code_dict__ = decompose_code(parameters, param_dict)\n",
    "        code_dict__['images'] = imgs_batch\n",
    "        return code_dict__\n",
    "\n",
    "def decompose_code(concatCode, num_dict__):\n",
    "        ''' Convert a flattened parameter vector to a dictionary of parameters\n",
    "        code_dict.keys() = [('shape',) 'tex', 'exp', 'pose', 'cam', 'light']\n",
    "        '''\n",
    "        code_dict_ = {}\n",
    "        start_ind_ = 0\n",
    "        for key in num_dict__:\n",
    "            end_ind___      = start_ind_+int(num_dict__[key])\n",
    "            code_dict_[key] = concatCode[:, start_ind_:end_ind___]\n",
    "            start_ind_      = end_ind___\n",
    "            if key == 'light':\n",
    "                code_dict_[key] = code_dict_[key].reshape(code_dict_[key].shape[0], 9, 3)\n",
    "        return code_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decalib.deca import DECA\n",
    "from decalib.datasets import datasets \n",
    "from decalib.utils import util\n",
    "from decalib.utils.config import cfg as deca_cfg\n",
    "from skimage.io import imread\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "image_path = \"/home/dietrich/Testing/DECA/DECA/TestSamples/Brenken_Rudolf/2017-10-24_00-00_Brenken-Rudolf_06/2017-10-24_001.jpg\"\n",
    "# Initialization\n",
    "deca = DECA()\n",
    "device = 'cuda'\n",
    "#Get the data in the right format\n",
    "test_img = datasets.TestData(image_path)\n",
    "#Get the codedictionarys\n",
    "for i in tqdm(range(len(test_img))):\n",
    "    name = test_img[i]['imagename']\n",
    "    images_l = test_img[i]['image'].to(device)[None,...]\n",
    "    with torch.no_grad(): \n",
    "        codedict = deca.encode(images_l)\n",
    "        shape_params = get_shape_params(image_path)\n",
    "        opdict, visdict = deca.decode(codedict, train_bool=True, shape_params=shape_params) #tensor\n",
    "        deca.save_obj(name + '_without_shape.obj', opdict)\n",
    "        print(codedict['exp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deca-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
