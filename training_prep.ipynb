{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for training with decoupled shape parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decalib.deca import DECA\n",
    "from decalib.datasets import datasets \n",
    "from decalib.utils import util\n",
    "from decalib.utils.config import cfg as deca_cfg\n",
    "from skimage.io import imread\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import cv2\n",
    "import trimesh\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get the Shape parameters for an image -> we need this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_params(image_path: str, device='cuda'):\n",
    "    \"\"\"Function for getting the shape paramters of an image\n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    deca = DECA()\n",
    "\n",
    "    #Get the data in the right format\n",
    "    test_img = datasets.TestData(image_path)\n",
    "    deca_cfg.model.use_tex = False\n",
    "    deca_cfg.model.extract_tex = False\n",
    "    #Get the codedictionarys\n",
    "    for i in tqdm(range(len(test_img))):\n",
    "        name = test_img[i]['imagename']\n",
    "        images_l = test_img[i]['image'].to(device)[None,...]\n",
    "        with torch.no_grad(): \n",
    "            codedict = deca.encode(images_l)\n",
    "            \n",
    "    return codedict['shape']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every image, we need to get the shape parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = '/home/dietrich/Testing/DECA/Dataset/VGGFACE/Images/VGGface2_None_norm_512_true_bygfpgan'\n",
    "lmks_folder = '/home/dietrich/Testing/DECA/Dataset/VGGFACE/lmks_train'\n",
    "shape_folder = '/home/dietrich/Testing/DECA/Dataset/VGGFACE/shape_train'\n",
    "img_name_list = []\n",
    "\n",
    "total_steps = 9279\n",
    "counter = 0\n",
    "\n",
    "progress_slider = widgets.IntSlider(min=0, max=total_steps, value=counter, description='Progress')\n",
    "logging.basicConfig(filename='/home/dietrich/Testing/DECA/Dataset/VGGFACE/lmks_and_shape_processing_6_imgs.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def update_slider(value):\n",
    "    progress_slider.value = value\n",
    "display(progress_slider)\n",
    "\n",
    "lms_478_to_68 = [\n",
    "                162,234,93,58,172,136,149,148,152,377,378,365,397,288,323,454,389,71,63,105,66,107,336,\n",
    "                296,334,293,301,168,197,5,4,75,97,2,326,305,33,160,158,133,153,144,362,385,387,263,373,\n",
    "                380,61,39,37,0,267,269,291,405,314,17,84,181,78,82,13,312,308,317,14,87\n",
    "            ]\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "                static_image_mode = True,\n",
    "                max_num_faces = 1,\n",
    "                refine_landmarks=False,\n",
    "                )\n",
    "\n",
    "expected_landmarks_count = 34 # Half should be detected\n",
    "# Through all folders in the Dataset Path\n",
    "for individ__name in os.listdir(images_folder):\n",
    "    individ__path = os.path.join(images_folder, individ__name)\n",
    "\n",
    "    if os.path.isdir(individ__path):\n",
    "        lmks_individ_path = os.path.join(lmks_folder, individ__name)\n",
    "        if not os.path.exists(lmks_individ_path):\n",
    "            os.makedirs(lmks_individ_path)\n",
    "        # e.g. K - number of images per person\n",
    "        img_num = 6\n",
    "        # counting valid images and break, if enough images are taken\n",
    "        img_list_individual = []\n",
    "        # For one individual (one folder) going through the images, until 6 are detected\n",
    "        for image_name in os.listdir(individ__path):\n",
    "            image_path = os.path.join(individ__path, image_name)\n",
    "\n",
    "            if image_name.endswith('.jpg'):\n",
    "                fn, ext = os.path.splitext(os.path.basename(image_name))\n",
    "\n",
    "                #Process Mediapipe\n",
    "                image = cv2.imread(image_path)\n",
    "                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('uint8')\n",
    "\n",
    "                results = face_mesh.process(image_rgb)\n",
    "                \n",
    "                # Excactly one person should be detected, else the image is not used\n",
    "                if results.multi_face_landmarks and len(results.multi_face_landmarks) == 1:\n",
    "                    landmarks = results.multi_face_landmarks[0].landmark\n",
    "                    selected_lm = [landmarks[i] for i in lms_478_to_68] \n",
    "                    selected_lm = [[lm.x * image.shape[1], lm.y * image.shape[0]] for lm in selected_lm]\n",
    "\n",
    "                    # Half of the face need to be detected, else another image is used\n",
    "                    if len(selected_lm) >= expected_landmarks_count:\n",
    "                        np.save(os.path.join(lmks_individ_path, fn + \"_68kpts.npy\") ,selected_lm)\n",
    "                        img_list_individual.append(os.path.join(individ__name, fn))\n",
    "                    else:\n",
    "                        logging.info(f'Not used: Detected less than 35 Landmarks: {individ__path}/{image_name}')\n",
    "                else:\n",
    "                    logging.info(f'Not used: Detected none or more than one face: {individ__path}/{image_name}')\n",
    "\n",
    "            #If we got 6 Images, the next individual will be landmarked\n",
    "            if len(img_list_individual) == img_num:\n",
    "                img_name_list.append(img_list_individual)\n",
    "                np.save(\"/home/dietrich/Testing/DECA/Dataset/VGGFACE/\"+\"data_names_6_per_individual\", img_name_list)\n",
    "                break\n",
    "    counter +=1\n",
    "    update_slider(counter)\n",
    "    \n",
    "print(\"Landmark Detection and Image Selection Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, if getting shape parameters is working the right way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = '/home/dietrich/Testing/DECA/Dataset/VGGFACE/data_names_6_per_individual.npy'\n",
    "data_lines = np.load(datafile).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_list = []\n",
    "base_path = '/home/dietrich/Testing/DECA/Dataset/VGGFACE/Images/VGGface2_None_norm_512_true_bygfpgan'\n",
    "\n",
    "idx = 0\n",
    "for i in range(6):\n",
    "    name = data_lines[idx, i]\n",
    "    image_path = os.path.join(base_path, name + '.jpg') \n",
    "    shape_params = get_shape_params(image_path)\n",
    "    shape_list.append(shape_params)\n",
    "\n",
    "shape_2d = torch.stack(shape_list)\n",
    "mean_shape = torch.mean(shape_2d, dim=0).cpu().detach().numpy()\n",
    "mean_shape_list = [mean_shape for _ in shape_list]\n",
    "\n",
    "\n",
    "shapearray = torch.from_numpy(np.array(mean_shape_list)).type(dtype = torch.float32) #K,100\n",
    "\n",
    "print(shapearray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decalib.deca import DECA\n",
    "from decalib.trainer import Trainer\n",
    "from decalib.utils.config import cfg\n",
    "import yaml\n",
    "import shutil\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "import torch\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decalib.models.encoders import ResnetEncoder\n",
    "import numpy as np\n",
    "from decalib.datasets import datasets \n",
    "import torch\n",
    "import os\n",
    "from decalib.utils import util\n",
    "\n",
    "n_shape = 100\n",
    "n_tex = 50\n",
    "n_exp = 50\n",
    "n_pose = 6\n",
    "n_cam = 3\n",
    "n_light = 27\n",
    "\n",
    "model_path = \"/home/dietrich/Testing/DECA/DECA/data/deca_model.tar\"\n",
    "num_params = n_shape+n_tex+n_exp+n_pose+n_cam+n_light\n",
    "E_flame  = ResnetEncoder(outsize=num_params).to('cuda')\n",
    "param_dict = {'shape': n_shape, 'tex': n_tex, 'exp': n_exp, 'pose': n_pose, 'cam': n_cam, 'light': n_light}\n",
    "checkpoint = torch.load(model_path)\n",
    "util.copy_state_dict(E_flame.state_dict(), checkpoint['E_flame'])\n",
    "E_flame.eval()\n",
    "\n",
    "def encode(imgs_batch):\n",
    "        with torch.no_grad():\n",
    "                parameters = E_flame(imgs_batch)\n",
    "        code_dict__ = decompose_code(parameters, param_dict)\n",
    "        code_dict__['images'] = imgs_batch\n",
    "        return code_dict__\n",
    "\n",
    "def decompose_code(concatCode, num_dict__):\n",
    "        ''' Convert a flattened parameter vector to a dictionary of parameters\n",
    "        code_dict.keys() = [('shape',) 'tex', 'exp', 'pose', 'cam', 'light']\n",
    "        '''\n",
    "        code_dict_ = {}\n",
    "        start_ind_ = 0\n",
    "        for key in num_dict__:\n",
    "            end_ind___      = start_ind_+int(num_dict__[key])\n",
    "            code_dict_[key] = concatCode[:, start_ind_:end_ind___]\n",
    "            start_ind_      = end_ind___\n",
    "            if key == 'light':\n",
    "                code_dict_[key] = code_dict_[key].reshape(code_dict_[key].shape[0], 9, 3)\n",
    "        return code_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = np.load('/home/dietrich/Testing/DECA/Dataset/VGGFACE/data_names_6_per_individual.npy').astype('str')\n",
    "image_folder = '/home/dietrich/Testing/DECA/Dataset/VGGFACE/Images/VGGface2_None_norm_512_true_bygfpgan'\n",
    "shape_folder = '/home/dietrich/Testing/DECA/Dataset/VGGFACE/shape_train'\n",
    "shape_mean_folder = '/home/dietrich/Testing/DECA/Dataset/VGGFACE/shape_train_mean'\n",
    "\n",
    "imagepath = [image_folder + '/' + data_name + '.jpg' for data_name in data_list[0, :]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Shape Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=0\n",
    "for idx in range(data_list.shape[0]):\n",
    "    folder_path = os.path.join(shape_folder, data_list[idx, 0].split('/')[0])\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    folder_path_mean = os.path.join(shape_mean_folder, data_list[idx, 0].split('/')[0])\n",
    "    if not os.path.exists(folder_path_mean):\n",
    "        os.makedirs(folder_path_mean)\n",
    "    imagepath = [image_folder + '/' + data_name + '.jpg' for data_name in data_list[idx, :]]\n",
    "    testdata__ = datasets.TestData(imagepath)\n",
    "    shape_array = []\n",
    "    for i in range(len(imagepath)):\n",
    "        imgs_batch = testdata__[i]['image'].to('cuda')[None,...]\n",
    "        codedict = encode(imgs_batch)\n",
    "        np.save(shape_folder + '/' + data_list[idx, i] + '.npy', codedict['shape'].cpu().numpy())\n",
    "        shape_array.append(codedict['shape'].cpu().numpy())\n",
    "    mean_shape = np.mean(np.array(shape_array), axis=0)\n",
    "        \n",
    "    np.save(shape_mean_folder + '/' + data_list[idx, 0].split('/')[0] + '/shape_mean.npy', mean_shape)\n",
    "    num+=1\n",
    "    print(num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_list = np.load('/home/dietrich/Testing/DECA/Dataset/VGGFACE/data_names_6_per_individual.npy').astype('str')\n",
    "\n",
    "data_list_valid = data_list[:20, :]\n",
    "\n",
    "data_list_train = data_list[20:, :]\n",
    "\n",
    "np.save('/home/dietrich/Testing/DECA/Dataset/VGGFACE/data_names_6_per_individual_train.npy', data_list_train)\n",
    "\n",
    "np.save('/home/dietrich/Testing/DECA/Dataset/VGGFACE/data_names_6_per_individual_valid.npy', data_list_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec $SHELL\n",
    "\n",
    "sbatch train.sh\n",
    "\n",
    "squeue #Aktueller stand"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
